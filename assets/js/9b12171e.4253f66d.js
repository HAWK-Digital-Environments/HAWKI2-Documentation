"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[705],{6869:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"HAWKI/Architecture/Model Connection","title":"Model Connection","description":"This document describes the architecture and implementation of HAWKI\'s AI model connection system, including the data flow, components, and how to add new AI providers.","source":"@site/docs/HAWKI/3-Architecture/6-Model Connection.md","sourceDirName":"HAWKI/3-Architecture","slug":"/HAWKI/Architecture/Model Connection","permalink":"/HAWKI2-Documentation/HAWKI/Architecture/Model Connection","draft":false,"unlisted":false,"editUrl":"https://github.com/hawk/your-project/edit/main/docs/HAWKI/3-Architecture/6-Model Connection.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"HAWKI_Sidebar","previous":{"title":"Encryption","permalink":"/HAWKI2-Documentation/HAWKI/Architecture/Encryption"},"next":{"title":"Broadcasting","permalink":"/HAWKI2-Documentation/HAWKI/Architecture/Broadcasting"}}');var s=r(4848),o=r(8453);const t={},i="Model Connection",l={},d=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Key Components",id:"key-components",level:2},{value:"Controller Layer",id:"controller-layer",level:3},{value:"Service Layer",id:"service-layer",level:3},{value:"Provider Layer",id:"provider-layer",level:3},{value:"Data Flow",id:"data-flow",level:2},{value:"Request Flow",id:"request-flow",level:3},{value:"Request Payload Structure",id:"request-payload-structure",level:3},{value:"Response Structure",id:"response-structure",level:3},{value:"Provider Implementation",id:"provider-implementation",level:2},{value:"Provider Interface",id:"provider-interface",level:3},{value:"Base Provider",id:"base-provider",level:3},{value:"Provider Examples",id:"provider-examples",level:3},{value:"OpenAI Provider",id:"openai-provider",level:4},{value:"Google Provider",id:"google-provider",level:4},{value:"How to Add a New Provider",id:"how-to-add-a-new-provider",level:2},{value:"1. Create a New Provider Class",id:"1-create-a-new-provider-class",level:3},{value:"2. Update the Provider Factory",id:"2-update-the-provider-factory",level:3},{value:"3. Update Configuration",id:"3-update-configuration",level:3},{value:"4. Provider-Specific Considerations",id:"4-provider-specific-considerations",level:3},{value:"5. Testing Your Provider",id:"5-testing-your-provider",level:3},{value:"Streaming vs Non-Streaming Requests",id:"streaming-vs-non-streaming-requests",level:2},{value:"Non-Streaming Requests",id:"non-streaming-requests",level:3},{value:"Streaming Requests",id:"streaming-requests",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Usage Analytics",id:"usage-analytics",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"model-connection",children:"Model Connection"})}),"\n",(0,s.jsx)(n.p,{children:"This document describes the architecture and implementation of HAWKI's AI model connection system, including the data flow, components, and how to add new AI providers."}),"\n",(0,s.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#architecture-overview",children:"Architecture Overview"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#key-components",children:"Key Components"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#data-flow",children:"Data Flow"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#provider-implementation",children:"Provider Implementation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#how-to-add-a-new-provider",children:"How to Add a New Provider"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#streaming-vs-non-streaming-requests",children:"Streaming vs Non-Streaming Requests"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#error-handling",children:"Error Handling"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"#usage-analytics",children:"Usage Analytics"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:"HAWKI's AI integration uses a service-based architecture to process requests to various AI models (OpenAI, GWDG, Google). The system follows a factory and strategy pattern to abstract the connection to different AI service providers while maintaining a consistent interface."}),"\n",(0,s.jsx)(n.p,{children:"Key features include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Support for multiple AI providers (OpenAI, Google, GWDG)"}),"\n",(0,s.jsx)(n.li,{children:"Both streaming and non-streaming response handling"}),"\n",(0,s.jsx)(n.li,{children:"Standardized interface for all providers"}),"\n",(0,s.jsx)(n.li,{children:"Extensible design for adding new providers"}),"\n",(0,s.jsx)(n.li,{children:"Usage tracking and analytics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-components",children:"Key Components"}),"\n",(0,s.jsx)(n.p,{children:"The AI connection system consists of the following key components:"}),"\n",(0,s.jsx)(n.h3,{id:"controller-layer",children:"Controller Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"StreamController"}),": Entry point for AI requests handling both direct and group chat interactions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"service-layer",children:"Service Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AIConnectionService"}),": Core orchestration service that manages the connection process"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AIProviderFactory"}),": Factory class that creates appropriate provider instances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"UsageAnalyzerService"}),": Tracks and records token usage for analytics and billing"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"provider-layer",children:"Provider Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AIModelProviderInterface"}),": Interface that all AI providers must implement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BaseAIModelProvider"}),": Abstract base class with common functionality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider Implementations"}),": Concrete implementations for each AI service (OpenAI, GWDG, Google)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"data-flow",children:"Data Flow"}),"\n",(0,s.jsx)(n.h3,{id:"request-flow",children:"Request Flow"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Client sends request to ",(0,s.jsx)(n.code,{children:"StreamController->handleAiConnectionRequest"})]}),"\n",(0,s.jsx)(n.li,{children:"Controller validates the request and extracts payload"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"AIConnectionService"})," processes the request"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"AIProviderFactory"})," creates the appropriate provider"]}),"\n",(0,s.jsx)(n.li,{children:"Provider formats the payload according to service requirements"}),"\n",(0,s.jsx)(n.li,{children:"Provider connects to the AI service API"}),"\n",(0,s.jsx)(n.li,{children:"Provider formats the response"}),"\n",(0,s.jsxs)(n.li,{children:["Usage is tracked in ",(0,s.jsx)(n.code,{children:"UsageAnalyzerService"})]}),"\n",(0,s.jsx)(n.li,{children:"Response is returned to the client"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"request-payload-structure",children:"Request Payload Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"$validatedData = $request->validate([\n    'payload.model' => 'required|string',\n    'payload.stream' => 'required|boolean',\n    'payload.messages' => 'required|array',\n    'payload.messages.*.role' => 'required|string',\n    'payload.messages.*.content' => 'required|array',\n    'payload.messages.*.content.text' => 'required|string',\n    \n    'broadcast' => 'required|boolean',\n    'isUpdate' => 'nullable|boolean',\n    'messageId' => 'nullable|string',\n    'threadIndex' => 'nullable|int', \n    'slug' => 'nullable|string',\n    'key' => 'nullable|string',\n]);\n"})}),"\n",(0,s.jsx)(n.h3,{id:"response-structure",children:"Response Structure"}),"\n",(0,s.jsx)(n.p,{children:"For non-streaming responses:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"[\n    'content' => 'Response text from AI model',\n    'usage' => [\n        'prompt_tokens' => 123,\n        'completion_tokens' => 456\n    ]\n]\n"})}),"\n",(0,s.jsx)(n.p,{children:"For streaming responses (per chunk):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"[\n    'content' => 'Partial response text',\n    'isDone' => false,\n    'usage' => null\n]\n"})}),"\n",(0,s.jsx)(n.h2,{id:"provider-implementation",children:"Provider Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Each AI provider follows the same interface but implements provider-specific handling."}),"\n",(0,s.jsx)(n.h3,{id:"provider-interface",children:"Provider Interface"}),"\n",(0,s.jsxs)(n.p,{children:["All providers must implement the ",(0,s.jsx)(n.code,{children:"AIModelProviderInterface"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"interface AIModelProviderInterface\n{\n    public function formatPayload(array $rawPayload): array;\n    public function formatResponse($response): array;\n    public function formatStreamChunk(string $chunk): array;\n    public function connect(array $payload, ?callable $streamCallback = null);\n    public function makeNonStreamingRequest(array $payload);\n    public function makeStreamingRequest(array $payload, callable $streamCallback);\n    public function getModelDetails(string $modelId): array;\n    public function supportsStreaming(string $modelId): bool;\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"base-provider",children:"Base Provider"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"BaseAIModelProvider"})," abstract class provides common functionality:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"abstract class BaseAIModelProvider implements AIModelProviderInterface\n{\n    protected $config;\n    \n    public function __construct(array $config)\n    {\n        $this->config = $config;\n    }\n    \n    public function connect(array $payload, ?callable $streamCallback = null)\n    {\n        $modelId = $payload['model'];\n        \n        if ($streamCallback && $this->supportsStreaming($modelId)) {\n            return $this->makeStreamingRequest($payload, $streamCallback);\n        } else {\n            return $this->makeNonStreamingRequest($payload);\n        }\n    }\n    \n    // Other common methods...\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"provider-examples",children:"Provider Examples"}),"\n",(0,s.jsx)(n.h4,{id:"openai-provider",children:"OpenAI Provider"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"class OpenAIProvider extends BaseAIModelProvider\n{\n    public function formatPayload(array $rawPayload): array\n    {\n        // Transform payload to OpenAI format\n    }\n    \n    public function formatResponse($response): array\n    {\n        // Extract content and usage from OpenAI response\n    }\n    \n    // Other implemented methods...\n}\n"})}),"\n",(0,s.jsx)(n.h4,{id:"google-provider",children:"Google Provider"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"class GoogleProvider extends BaseAIModelProvider\n{\n    public function formatPayload(array $rawPayload): array\n    {\n        // Transform payload to Google Gemini format\n    }\n    \n    public function formatResponse($response): array\n    {\n        // Extract content and usage from Google response\n    }\n    \n    // Other implemented methods...\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"how-to-add-a-new-provider",children:"How to Add a New Provider"}),"\n",(0,s.jsx)(n.p,{children:"Adding a new AI provider to HAWKI is a straightforward process that involves creating a new provider class and updating the configuration. Follow these steps:"}),"\n",(0,s.jsx)(n.h3,{id:"1-create-a-new-provider-class",children:"1. Create a New Provider Class"}),"\n",(0,s.jsxs)(n.p,{children:["Create a new class in the ",(0,s.jsx)(n.code,{children:"app/Services/AI/Providers"})," directory that extends ",(0,s.jsx)(n.code,{children:"BaseAIModelProvider"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"<?php\n\nnamespace App\\Services\\AI\\Providers;\n\nuse Illuminate\\Support\\Facades\\Log;\n\nclass OllamaProvider extends BaseAIModelProvider\n{\n    /**\n     * Format the raw payload for Ollama API\n     *\n     * @param array $rawPayload\n     * @return array\n     */\n    public function formatPayload(array $rawPayload): array\n    {\n        $messages = $rawPayload['messages'];\n        $modelId = $rawPayload['model'];\n        \n        // Format messages for Ollama\n        $formattedMessages = [];\n        foreach ($messages as $message) {\n            $formattedMessages[] = [\n                'role' => $message['role'],\n                'content' => $message['content']['text']\n            ];\n        }\n        \n        return [\n            'model' => $modelId,\n            'messages' => $formattedMessages,\n            'stream' => $rawPayload['stream'] && $this->supportsStreaming($modelId),\n        ];\n    }\n    \n    /**\n     * Format the complete response from Ollama\n     *\n     * @param mixed $response\n     * @return array\n     */\n    public function formatResponse($response): array\n    {\n        $responseContent = $response->getContent();\n        $jsonContent = json_decode($responseContent, true);\n        \n        // Extract content based on Ollama's response format\n        $content = $jsonContent['response'] ?? '';\n        \n        return [\n            'content' => $content,\n            'usage' => $this->extractUsage($jsonContent)\n        ];\n    }\n    \n    /**\n     * Format a single chunk from a streaming response\n     *\n     * @param string $chunk\n     * @return array\n     */\n    public function formatStreamChunk(string $chunk): array\n    {\n        $jsonChunk = json_decode($chunk, true);\n        \n        $content = '';\n        $isDone = false;\n        $usage = null;\n        \n        // Extract content based on Ollama's streaming format\n        if (isset($jsonChunk['response'])) {\n            $content = $jsonChunk['response'];\n        }\n        \n        // Check if this is the final chunk\n        if (isset($jsonChunk['done']) && $jsonChunk['done'] === true) {\n            $isDone = true;\n            \n            // Extract usage if available in the final chunk\n            if (isset($jsonChunk['eval_count']) && isset($jsonChunk['prompt_eval_count'])) {\n                $usage = [\n                    'prompt_tokens' => $jsonChunk['prompt_eval_count'],\n                    'completion_tokens' => $jsonChunk['eval_count'] - $jsonChunk['prompt_eval_count'],\n                ];\n            }\n        }\n        \n        return [\n            'content' => $content,\n            'isDone' => $isDone,\n            'usage' => $usage\n        ];\n    }\n    \n    /**\n     * Extract usage information from Ollama response\n     *\n     * @param array $data\n     * @return array|null\n     */\n    protected function extractUsage(array $data): ?array\n    {\n        if (!isset($data['eval_count']) || !isset($data['prompt_eval_count'])) {\n            return null;\n        }\n        \n        return [\n            'prompt_tokens' => $data['prompt_eval_count'],\n            'completion_tokens' => $data['eval_count'] - $data['prompt_eval_count'],\n        ];\n    }\n    \n    /**\n     * Make a non-streaming request to the Ollama API\n     *\n     * @param array $payload\n     * @return mixed\n     */\n    public function makeNonStreamingRequest(array $payload)\n    {\n        // Ensure stream is set to false\n        $payload['stream'] = false;\n        \n        // Initialize cURL\n        $ch = curl_init();\n        curl_setopt($ch, CURLOPT_URL, $this->config['api_url']);\n        \n        // Set common cURL options\n        $this->setCommonCurlOptions($ch, $payload, $this->getHttpHeaders());\n        \n        // Execute the request\n        $response = curl_exec($ch);\n        \n        // Handle errors\n        if (curl_errno($ch)) {\n            $error = 'Error: ' . curl_error($ch);\n            curl_close($ch);\n            return response()->json(['error' => $error], 500);\n        }\n        \n        curl_close($ch);\n        \n        return response($response)->header('Content-Type', 'application/json');\n    }\n    \n    /**\n     * Make a streaming request to the Ollama API\n     *\n     * @param array $payload\n     * @param callable $streamCallback\n     * @return void\n     */\n    public function makeStreamingRequest(array $payload, callable $streamCallback)\n    {\n        // Implementation of streaming request for Ollama\n        // Similar to OpenAI implementation but adapted for Ollama's API\n        \n        // Ensure stream is set to true\n        $payload['stream'] = true;\n        \n        set_time_limit(120);\n        \n        // Set headers for SSE\n        header('Content-Type: text/event-stream');\n        header('Cache-Control: no-cache');\n        header('Connection: keep-alive');\n        header('Access-Control-Allow-Origin: *');\n        \n        // Initialize cURL\n        $ch = curl_init();\n        curl_setopt($ch, CURLOPT_URL, $this->config['api_url']);\n        \n        // Set common cURL options\n        $this->setCommonCurlOptions($ch, $payload, $this->getHttpHeaders(true));\n        \n        // Set streaming-specific options\n        $this->setStreamingCurlOptions($ch, $streamCallback);\n        \n        // Execute the cURL session\n        curl_exec($ch);\n        \n        // Handle errors\n        if (curl_errno($ch)) {\n            $streamCallback('Error: ' . curl_error($ch));\n            if (ob_get_length()) {\n                ob_flush();\n            }\n            flush();\n        }\n        \n        curl_close($ch);\n        \n        // Flush any remaining data\n        if (ob_get_length()) {\n            ob_flush();\n        }\n        flush();\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-update-the-provider-factory",children:"2. Update the Provider Factory"}),"\n",(0,s.jsxs)(n.p,{children:["Update the ",(0,s.jsx)(n.code,{children:"AIProviderFactory"})," class to include your new provider:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"public function getProviderForModel(string $modelId): AIModelProviderInterface\n{\n    $providerId = $this->getProviderId($modelId);\n    \n    switch ($providerId) {\n        case 'openai':\n            return new OpenAIProvider($this->config['providers']['openai']);\n        case 'gwdg':\n            return new GWDGProvider($this->config['providers']['gwdg']);\n        case 'google':\n            return new GoogleProvider($this->config['providers']['google']);\n        case 'ollama':\n            return new OllamaProvider($this->config['providers']['ollama']);\n        default:\n            throw new \\Exception(\"Unsupported provider: {$providerId}\");\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-update-configuration",children:"3. Update Configuration"}),"\n",(0,s.jsxs)(n.p,{children:["Add your new provider to the ",(0,s.jsx)(n.code,{children:"config/model_providers.php"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"'ollama' => [\n    'id' => 'ollama',\n    'active' => true,\n    'api_key' => '', // If needed\n    'api_url' => 'http://localhost:11434/api/chat',\n    'ping_url' => 'http://localhost:11434/api/tags',\n    'models' => [\n        [\n            'id' => 'llama3',\n            'label' => 'Ollama Llama 3',\n            'streamable' => true,\n        ],\n        [\n            'id' => 'mistral',\n            'label' => 'Ollama Mistral',\n            'streamable' => true,\n        ],\n    ]\n]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"4-provider-specific-considerations",children:"4. Provider-Specific Considerations"}),"\n",(0,s.jsx)(n.p,{children:"When implementing a new provider, consider these aspects:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Format Differences"}),": Understand how the API expects messages and returns responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming Protocol"}),": Implement the correct streaming protocol for the provider"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Usage Tracking"}),": Extract token usage information correctly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Handle provider-specific error responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Capabilities"}),": Configure which models support streaming"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-testing-your-provider",children:"5. Testing Your Provider"}),"\n",(0,s.jsx)(n.p,{children:"After implementing your provider, test it thoroughly:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Test non-streaming requests"}),"\n",(0,s.jsx)(n.li,{children:"Test streaming requests"}),"\n",(0,s.jsx)(n.li,{children:"Verify error handling"}),"\n",(0,s.jsx)(n.li,{children:"Check usage tracking"}),"\n",(0,s.jsx)(n.li,{children:"Test with different message inputs"}),"\n",(0,s.jsx)(n.li,{children:"Validate response formatting"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"streaming-vs-non-streaming-requests",children:"Streaming vs Non-Streaming Requests"}),"\n",(0,s.jsx)(n.p,{children:"HAWKI's model connection system supports both streaming and non-streaming requests."}),"\n",(0,s.jsx)(n.h3,{id:"non-streaming-requests",children:"Non-Streaming Requests"}),"\n",(0,s.jsx)(n.p,{children:"Non-streaming requests wait for the complete response before returning to the client:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"// In AIConnectionService\npublic function processRequest(array $rawPayload, bool $streaming = false, ?callable $streamCallback = null)\n{\n    $modelId = $rawPayload['model'];\n    $provider = $this->providerFactory->getProviderForModel($modelId);\n    \n    // Format the payload\n    $formattedPayload = $provider->formatPayload($rawPayload);\n    \n    if (!$streaming) {\n        // Standard request (non-streaming)\n        $response = $provider->connect($formattedPayload);\n        return $provider->formatResponse($response);\n    }\n    \n    // Streaming handled elsewhere...\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"streaming-requests",children:"Streaming Requests"}),"\n",(0,s.jsx)(n.p,{children:"Streaming requests send partial responses to the client as they become available:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"// In StreamController\nprivate function handleStreamingRequest(array $payload, User $user, ?string $avatar_url)\n{\n    // Set headers for SSE\n    header('Content-Type: text/event-stream');\n    header('Cache-Control: no-cache');\n    header('Connection: keep-alive');\n    header('Access-Control-Allow-Origin: *');\n    \n    // Create a callback function to process streaming chunks\n    $onData = function ($data) use ($user, $avatar_url, $payload) {\n        // Format and send chunks to client\n    };\n    \n    // Process the streaming request\n    $this->aiConnectionService->processRequest(\n        $payload, \n        true, \n        $onData\n    );\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsx)(n.p,{children:"The system includes error handling at multiple levels:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Validation"}),": The controller validates all incoming requests"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider Selection"}),": The factory validates model IDs against available providers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Connection Errors"}),": cURL connection errors are caught and reported"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Parsing"}),": JSON parsing errors are handled gracefully"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming Disconnections"}),": Connection aborts are detected and handled"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example error handling:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"try {\n    $provider = $this->providerFactory->getProviderForModel($modelId);\n    $formattedPayload = $provider->formatPayload($rawPayload);\n    $response = $provider->connect($formattedPayload);\n} catch (\\Exception $e) {\n    Log::error('AI connection error: ' . $e->getMessage());\n    return response()->json(['error' => 'Failed to connect to AI service'], 500);\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"usage-analytics",children:"Usage Analytics"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"UsageAnalyzerService"})," tracks AI model usage for analytics and billing:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-js",children:"public function submitUsageRecord($usage, $type, $model, $roomId = null) {\n    $today = Carbon::today();\n    $userId = Auth::user()->id;\n\n    // Create a new record\n    UsageRecord::create([\n        'user_id' => $userId,\n        'room_id' => $roomId,\n        'prompt_tokens' => $usage['prompt_tokens'],\n        'completion_tokens' => $usage['completion_tokens'],\n        'model' => $model,\n        'type' => $type,\n    ]);\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"This data can be used for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitoring usage patterns"}),"\n",(0,s.jsx)(n.li,{children:"Cost allocation"}),"\n",(0,s.jsx)(n.li,{children:"Setting usage limits"}),"\n",(0,s.jsx)(n.li,{children:"Generating reports"}),"\n",(0,s.jsx)(n.li,{children:"Optimizing model selection"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>i});var a=r(6540);const s={},o=a.createContext(s);function t(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);